{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3452688-7f78-427f-aabf-003ad15b10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment KNN & PCA(Machine Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553c5d97-d4f9-4051-87d0-990664df0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNO1 What is K-Nearest Neighbours (KNN) and how does it workin both classification and regression problems?\n",
    "# Ans:K-Nearest Neighbours (KNN) is a supervised, non-parametric, instance-based (lazy) machine learning algorithm.\n",
    "#It does not build an explicit model during training. Instead, it stores the training data and makes predictions based on similarity (distance) \n",
    "#between data points.\n",
    "# KNN Works (Common Steps)\n",
    "#Choose the value of K (number of neighbours)\n",
    "#Select a distance metric\n",
    "#Euclidean (most common)\n",
    "#Manhattan\n",
    "#Minkowski\n",
    "#Calculate distances between the new point and all training points\n",
    "#Select K nearest neighbours\n",
    "#Make prediction\n",
    "#Voting → Classification\n",
    "#Averaging → Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9e214b-fc8b-4ec4-a671-c51ba0517ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qno2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
    "#Ans:The Curse of Dimensionality refers to a set of problems that arise when the number of features (dimensions) in a dataset becomes very large.\n",
    "#As dimensions increase, the data becomes sparse, distances lose meaning, and many machine learning algorithms—especially distance-based models like KNN—perform poorly.\n",
    "#Effect of Curse of Dimensionality on KNN\n",
    "#KNN relies completely on distance calculations. When dimensions increase:\n",
    "#1. Distances Become Similar\n",
    "#2. Nearest Neighbour is Far Away\n",
    "#3. Increased Noise Impact\n",
    "#4. Higher Computational Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeb40b2e-c1a0-4ce8-a05c-00b01ecc4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qno3:What is the Principle Component Analysis(PCA)?How does it different from feature selection?\n",
    "#Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique used to reduce the number of features while retaining as much variance (information) as possible from the original data.\n",
    "#It transforms the original correlated features into a new set of uncorrelated variables called principal components.\n",
    "#How PCA Works (Conceptually)\n",
    "#Standardize the data\n",
    "#Compute covariance matrix\n",
    "#Find eigenvalues and eigenvectors\n",
    "#Sort eigenvectors by descending eigenvalues\n",
    "#Select top k components\n",
    "#Project data onto these components\n",
    "#Main Difference: PCA vs Feature Selection\n",
    "#| Aspect                      | PCA                               | Feature Selection          |\n",
    "#| --------------------------- | --------------------------------- | -------------------------- |\n",
    "#| Type                        | Feature **extraction**            | Feature **selection**      |\n",
    "#| New features created?       |  Yes (principal components)      |  No                       |\n",
    "#| Original features retained? |  No (transformed)                | Yes                      |\n",
    "#| Supervised/Unsupervised     | Unsupervised                      | Often supervised           |\n",
    "#| Interpretability            | Low                               | High                       |\n",
    "#| Handles multicollinearity   | Yes                               | Partially                  |\n",
    "#| Information retained        | Maximizes variance                | Depends on selection       |\n",
    "#| Example                     | Combine age, weight, height → PC1 | Select only age and weight |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1215ddc7-3afc-4d24-a378-f15461e29c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qno4:What are the eignvalues and eigenvectors in PCA,and Why are they important?\n",
    "#An eigenvector is a direction in the feature space along which the data varies the most.\n",
    "#In PCA:\n",
    "#Each eigenvector = a principal component direction\n",
    "#They define new axes onto which data is projected\n",
    "#Eigenvectors are orthogonal (perpendicular) to each other\n",
    "#Intuitive Meaning\n",
    "#Eigenvectors tell us “which direction to look” to capture maximum information\n",
    "#An eigenvalue represents the amount of variance captured by its corresponding eigenvector.\n",
    "#In PCA:\n",
    "#Larger eigenvalue → more information\n",
    "#Smaller eigenvalue → less information (noise)\n",
    "#Intuitive Meaning\n",
    "#Eigenvalues tell us “how important that direction is.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb62ec-bf2d-4ee5-9f47-72c64313e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qno.5 How do KNN and PCA Complement each other when applied in a single pipeline?\n",
    "#KNN and PCA are often used together because PCA fixes many weaknesses of KNN, especially in high-dimensional data.\n",
    "#Together, they form an efficient, accurate, and robust pipeline.\n",
    "#Raw Data\n",
    "#   ↓\n",
    "#Feature Scaling (Standardization)\n",
    "#   ↓\n",
    "#PCA (Dimensionality Reduction)\n",
    "#   ↓\n",
    "#KNN (Classification / Regression)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
