{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504a8b14-8986-45ce-8b51-5b0e5dd4c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assignment5_ML_Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "315afb90-1710-4a39-98f6-bb732bf871e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qno.1 What is the difference between K-Means and Hierachical Clustering? Provide a use case of each\n",
    "#K-Means is a partition-based, centroid-driven clustering algorithm that divides data into K predefined clusters.\n",
    "#Key Characteristics\n",
    "#Requires K to be specified in advance\n",
    "#Works best with numerical data\n",
    "#Assumes spherical, equally sized clusters\n",
    "#Fast and scalable for large datasets\n",
    "#Sensitive to outliers and initial centroid selection\n",
    "#Use K-Means when:\n",
    "#Dataset is large\n",
    "#K is known\n",
    "#Features are numerical\n",
    "#Speed matters\n",
    "\n",
    "#Hierarchical clustering builds a tree-like structure (dendrogram) showing nested clusters without requiring K beforehand.\n",
    "#Key Characteristics\n",
    "#No need to specify K initially\n",
    "#Can work with numerical & categorical data\n",
    "#Provides interpretable hierarchy\n",
    "#Computationally expensive (not ideal for very large datasets)\n",
    "#Once merged/split, clusters cannot be undone\n",
    "#Use Hierarchical Clustering when:\n",
    "#Dataset is small to medium\n",
    "#K is unknown\n",
    "#You need interpretability\n",
    "#Exploring natural groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ee2a30-494f-4d53-b189-e241c0a153ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qno.2Explain the purpose of the Silhouette Score in Evaluating clustering algrithm?\n",
    "#Ans-The Silhouette Score is an internal validation metric used to evaluate how well a clustering algorithm has \n",
    "#grouped the data without using ground-truth labels.#Its main purpose is to measure cluster quality by considering both cohesion and separation.\n",
    "#For each data point \n",
    "#ùëñ, the silhouette score s(i) is defined as:\n",
    "#s(i)=b(i)‚àía(i)/max a(i)b(i)\n",
    "#Where:\n",
    "#a(i) = Average distance of point \n",
    "#i to all other points within the same cluster\n",
    "# Measures cluster cohesion (how well the point fits in its own cluster)\n",
    "#b(i) = Average distance of point \n",
    "#i to points in the nearest neighboring cluster\n",
    "# Measures cluster separation (how far it is from other clusters)\n",
    "#| Score Range         | Interpretation                                            |\n",
    "#| ------------------- | --------------------------------------------------------- |\n",
    "#| **Close to +1**     | Point is well clustered and far from neighboring clusters |\n",
    "#| **Around 0**        | Point lies near the boundary between clusters             |\n",
    "#| **Negative values** | Point is likely assigned to the wrong cluster             |\n",
    "#Advantages\n",
    "#No need for labeled data\n",
    "#Easy to interpret\n",
    "#Works with many clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1d63bc-e41c-4d23-b7d9-a5b8efe7147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qno3: What are the core parameters of DBSCAN nad how do they influence the clustering process?\n",
    "#Ans:DBSCAN (Density-Based Spatial Clustering of Applications with Noise) groups data points based on density, making it very effective for discovering arbitrarily shaped clusters and identifying outliers (noise).\n",
    "#DBSCAN has two core parameters and one derived concept that together control the clustering behavior.\n",
    "#eps (Œµ ‚Äì Epsilon):The maximum radius around a data point to search for neighboring points.\n",
    "#How it influences clustering\n",
    "#Small Œµ\n",
    "#Fewer neighbors\n",
    "#Many points labeled as noise\n",
    "#Clusters may fragment or not form at all\n",
    "#Large Œµ\n",
    "#More neighbors\n",
    "#Clusters may merge\n",
    "#Noise may be absorbed into clusters\n",
    "\n",
    "#min_samples (or MinPts):The minimum number of points (including the point itself) required within Œµ to form a dense region.\n",
    "#How it influences clustering\n",
    "#Low MinPts\n",
    "#More clusters\n",
    "#Sensitive to noise\n",
    "#Risk of forming clusters from random fluctuations\n",
    "#High MinPts\n",
    "#Fewer, denser clusters\n",
    "#More points classified as noise\n",
    "#Better noise robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90fb919d-a43f-4007-a633-7d8aba60211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qno4:Why is feature scaling important when applying clustering alogorithms like K means and DBSCAN?\n",
    "#Feature scaling is critical for distance-based clustering algorithms because they rely directly on distance calculations to form clusters.\n",
    "#Without scaling, features with larger numeric ranges can dominate the distance metric, leading to misleading clusters.\n",
    "#1. Distance Is the Core of Clustering\n",
    "#Both K-Means and DBSCAN typically use Euclidean distance (or similar metrics):\n",
    "#d(x,y)=‚àë(xi‚àíyi)2\n",
    "#If one feature has values in thousands (e.g., income) and another in single digits (e.g., age),\n",
    "#the larger-scale feature will overpower the smaller one‚Äîeven if it‚Äôs less important.\n",
    "#2. Importance of Feature Scaling in K-Means\n",
    "#Why K-Means is sensitive\n",
    "#Uses Euclidean distance to assign points to centroids\n",
    "#Centroids are calculated as means of feature values\n",
    "#Without scaling\n",
    "#High-magnitude features dominate centroid position\n",
    "#Clusters reflect scale, not structure\n",
    "#With scaling\n",
    "#All features contribute equally\n",
    "#Centroids represent balanced dimensions\n",
    "#3. Importance of Feature Scaling in DBSCAN\n",
    "#Why DBSCAN is sensitive\n",
    "#Uses distance to define Œµ-neighborhood\n",
    "#Density estimation depends on local distances\n",
    "#Without scaling\n",
    "#Œµ becomes meaningless\n",
    "#Dense regions may not be detected\n",
    "#Many points classified as noise or wrongly clustered\n",
    "#With scaling\n",
    "#Œµ represents a true neighborhood\n",
    "#Density-based clusters form correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012cdbd-308f-4dac-9105-b68b0a27edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QNO5:What is  the  Elbow Method in K-Means clustering and how does it help determine the optimum number of clusters?\n",
    "#he Elbow Method is a heuristic technique used to determine the optimal number of clusters (K) in K-Means clustering by analyzing how \n",
    "#the clustering error changes as K increases.\n",
    "#As K increases:\n",
    "#Clusters become smaller and more compact\n",
    "#Within-Cluster Sum of Squares (WCSS) decreases\n",
    "#Marginal improvement reduces after a certain K\n",
    "#That point of diminishing returns looks like an ‚Äúelbow‚Äù in the curve.\n",
    "#WCSS (also called inertia) measures how tightly the data points are grouped within clusters:\n",
    "#WCSS=k=1‚àëKxi‚ààCk‚àë‚à•xi‚àíŒºk‚à•2\n",
    "#Where:\n",
    "#ùê∂ùëò = cluster k\n",
    "#ùúáùëò = centroid of cluster k\n",
    "\n",
    "#Lower WCSS ‚Üí more compact clusters\n",
    "#Steps to Apply the Elbow Method\n",
    "#Choose a range of K values (e.g., 1 to 10)\n",
    "#Run K-Means for each K\n",
    "#Compute WCSS for each K\n",
    "#Plot K vs WCSS\n",
    "#Identify the point where the decrease in WCSS slows sharply (the elbow)\n",
    "#Determines the Optimal K\n",
    "#Before the elbow\n",
    "#‚Üí Adding clusters significantly reduces WCSS (useful improvement)\n",
    "#After the elbow\n",
    "#‚Üí Adding clusters gives minimal improvement (overfitting)\n",
    "#Advantages\n",
    "#Easy to understand and implement\n",
    "#Works well for spherical, well-separated clusters\n",
    "#Commonly used in practice\n",
    "#Limitations\n",
    "#Elbow may not be clearly visible\n",
    "#Subjective interpretation\n",
    "#Not reliable for overlapping or non-spherical clusters\n",
    "#Depends on feature scaling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
