{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779e2fd5-3795-4977-88b1-b58a2515bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assignment 3 SVM & Navi Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96818794-e8ab-4aea-bffc-89f9820de40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qno 1: \n",
    "#Ans:nformation Gain measures how much uncertainty (entropy) in the target variable is reduced after splitting the dataset based on a particular feature.\n",
    "#Higher Information Gain = better feature for splitting\n",
    "#Lower Information Gain = less useful feature\n",
    "#It helps the decision tree choose the feature that gives the purest child nodes.\n",
    "\n",
    "#Entropy (H):Entropy measures the degree of randomness or impurity in the datase\n",
    "#For binary classification:\n",
    "#H(S)=‚àíp1log2(p1)‚àíp2log2(p2)\n",
    "#Where\n",
    "\n",
    "#p1 = proportion of class 1\n",
    "#p2 = proportion of class 2\n",
    "#Information Gain (IG):IG(S,A)=H(S)‚àív‚ààValues(A)‚àë‚à£S‚à£‚à£Sv‚à£H(Sv)\n",
    "#where as \n",
    "#S = parent dataset\n",
    "#A = attribute (feature)\n",
    "#Sv = subset of data where feature ùê¥ has value ùë£\n",
    "#H(S) = entropy of parent\n",
    "#H(Sv) = entropy of child nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791db462-4059-4b3d-a42b-df30742f8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.What is the differencebeteen Gini Impurity and Entropy?\n",
    "#| Feature        | Gini Impurity          | Entropy                |\n",
    "#| -------------- | ---------------------- | ---------------------- |\n",
    "#| Formula        | (1 - \\sum p_i^2)       | (-\\sum p_i \\log_2 p_i) |\n",
    "#| Speed          | Faster (no logs)       | Slower (logs)          |\n",
    "#| Range (binary) | 0 ‚Äì 0.5                | 0 ‚Äì 1                  |\n",
    "#| Used in        | CART                   | ID3, C4.5              |\n",
    "#| Splits         | Picks dominant classes | Balanced splits        |\n",
    "#| Purity         | Slightly better purity | More theoretical rigor |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30df84e9-67e8-4f0a-b70f-62ca1dad35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. What is Pre-Pruning in Decision Tree?\n",
    "#Pre-Pruning (also called early stopping) is a technique used in Decision Trees to stop the tree from growing\n",
    "#too deep during training in order to avoid overfitting.\n",
    "#Decision Trees naturally grow very deep and try to perfectly classify training data.\n",
    "#This leads to:\n",
    "#Overfitting\n",
    "#Poor performance on unseen data\n",
    "#Complex, hard-to-interpret trees\n",
    "#Pre-pruning prevents this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc0e5adf-cb58-4560-8457-cbab13c506c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. What is a Support Vector Machine(SVM)?\n",
    "#Ans.A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression, \n",
    "#but it is most commonly used for binary classification.\n",
    "#Key Concepts\n",
    "#Hyperplane\n",
    "#A decision boundary that separates classes.\n",
    "#1D ‚Üí point\n",
    "#2D ‚Üí line\n",
    "#3D ‚Üí plane\n",
    "#nD ‚Üí hyperplane\n",
    "#Margin:Distance between hyperplane and the nearest data points from each class.\n",
    "# Support Vectors\n",
    "#The data points closest to the hyperplane.\n",
    "#These points define the boundary.\n",
    "#Types of SVM\n",
    "#1. Linear SVM\n",
    "#Used when the data is linearly separable.\n",
    "#2. Non-linear SVM\n",
    "#When data cannot be separated by a straight line.\n",
    "#Advantages of SVM\n",
    "#Works well with high-dimensional data\n",
    "#Effective when number of samples is small\n",
    "#Robust and accurate\n",
    "#Kernel trick allows solving complex problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95dc54a8-244e-43c2-9e6c-cb851ccb24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. What is the Kernal Trick in SVM?\n",
    "#Ans:The Kernel Trick is one of the most powerful ideas in Support Vector Machines (SVMs).\n",
    "#It allows SVMs to separate data that is not linearly separable by transforming it into a higher-dimensional\n",
    "#space‚Äîwithout actually computing that transformation.\n",
    "#The Kernel Trick lets SVM perform classification in a high-dimensional feature space without explicitly converting the data into that space.\n",
    "#Instead of computing the coordinates of the transformed data, SVM computes dot products using a kernel function.\n",
    "#Common Kernel Functions in SVM\n",
    "#Linear Kernel:K(x,y)=x‚ãÖy\n",
    "#Polynomial Kernel:K(x,y)=(x‚ãÖy+c)d\n",
    "#RBF (Radial Basis Function) Kernel / Gaussian:K(x,y)=exp(‚àíŒ≥‚à£‚à£x‚àíy‚à£‚à£2)\n",
    "#Sigmoid Kernel:K(x,y)=tanh(kx‚ãÖy+c)\n",
    "#Advantages of Kernel Trick\n",
    "# Allows SVM to solve non-linear problems\n",
    "# Avoids the cost of computing high-dimensional transformations\n",
    "# Very flexible due to many kernel choices\n",
    "# Creates very powerful decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1d4aa7d-e0ed-4805-b8b9-75f585c53e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. What is the Naive Bayes Classifier and why it is called Naive?\n",
    "#Ans:he Naive Bayes Classifier is a probabilistic machine learning algorithm used for classification tasks.\n",
    "#It is based on Bayes‚Äô Theorem and works especially well for text classification, spam detection, sentiment analysis, etc.\n",
    "#Naive Bayes uses probabilities of features belonging to a class to predict the most likely class.\n",
    "#It calculates:P(Class‚à£Features)\n",
    "#Using Bayes‚Äô Theorem:P(C‚à£X)=P(X)P(X‚à£C)‚ãÖP(C)\n",
    "#Where:\n",
    "#P(C‚à£X): Probability of class C given features X\n",
    "#P(X‚à£C): Probability of features X given class C\n",
    "#P(C): Prior probability of class\n",
    "#P(X): Prior probability of features\n",
    "#Why is it called ‚ÄúNaive‚Äù\n",
    "#Because the classifier makes a naive assumption:\n",
    "#It assumes that all features are independent of each other\n",
    "#given the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766bf12-a6b3-455d-b172-ccaca0aa6f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Explain the difference between Gaussian Naive Bayes, Multinomial Naive Bayes and Bernoulli Naive Bayes?\n",
    "# Ans:| Type               | Best For              | Feature Type   | Distribution Used | Example Use Case           |\n",
    "#| ------------------ | --------------------- | -------------- | ----------------- | -------------------------- |\n",
    "#| **Gaussian NB**    | Continuous data       | Real-valued    | Gaussian (Normal) | Medical diagnosis, sensors |\n",
    "#| **Multinomial NB** | Text with word counts | Integer counts | Multinomial       | Spam detection, NLP        |\n",
    "#| **Bernoulli NB**   | Binary features       | 0/1            | Bernoulli         | Presence/absence of words  |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
